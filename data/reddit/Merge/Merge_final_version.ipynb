{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62efdbb9",
   "metadata": {},
   "source": [
    "### Task 1: Data Ingestion, Union & Ticker Extraction\n",
    "\n",
    "This notebook implements a scalable pipeline to ingest, union, and extract stock tickers from the WallStreetBets subreddit data (posts and comments).  \n",
    "\n",
    "- **Purpose:** Parellely read compressed Reddit submission and comment archives, unify them into a single DataFrame, and extract valid stock tickers for downstream analysis.  \n",
    "\n",
    "- **Environment:** Developed and tested interactively in JupyterHub on an AWS EMR cluster using PySpark.  \n",
    "\n",
    "- **Data Location:** Raw `.zst` files were initially stored in an S3 bucket (`wsb-research-data-shef-20250522`). All the imported and generated data of this script, as well as subsequent  processing has been migrated to the Midway, where the remainder of the workflow will run. \n",
    " \n",
    "- **Workflow Details:** See the project README for a full description of data sources, directory structure, and end-to-end processing steps.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ad9555",
   "metadata": {},
   "source": [
    "#### 1.1 Spark session calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963eec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "import site, os\n",
    "\n",
    "JUPY_PY  = \"/usr/bin/python3\"\n",
    "SITE_DIR = site.getsitepackages()[0]\n",
    "ZSTD_JAR = \"/usr/lib/hadoop-yarn/share/hadoop/common/lib/hadoop-zstd.jar\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"WSB_ETL_JVM_Zstd\")\n",
    "        .master(\"yarn\")\n",
    "        # Python path\n",
    "        .config(\"spark.pyspark.python\",        JUPY_PY)\n",
    "        .config(\"spark.pyspark.driver.python\", JUPY_PY)\n",
    "        .config(\"spark.executorEnv.PYTHONPATH\", SITE_DIR)\n",
    "        # JVM and zstd\n",
    "        .config(\"spark.driver.extraClassPath\",   ZSTD_JAR)\n",
    "        .config(\"spark.executor.extraClassPath\", ZSTD_JAR)\n",
    "        .config(\"spark.hadoop.io.compression.codecs\",\n",
    "                \"org.apache.hadoop.io.compress.ZStandardCodec\")\n",
    "        # resource management\n",
    "        .config(\"spark.executor.instances\",      \"3\")\n",
    "        .config(\"spark.executor.cores\",          \"4\")\n",
    "        .config(\"spark.executor.memory\",         \"20g\")\n",
    "        .config(\"spark.executor.memoryOverhead\", \"4g\")\n",
    "        .config(\"spark.driver.memory\",           \"8g\")\n",
    "        # Shuffle / AQE\n",
    "        .config(\"spark.sql.adaptive.enabled\",   \"true\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"120\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafc00ba",
   "metadata": {},
   "source": [
    "#### 1.2 Extraction helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16712e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ─── Cell 2 : Reader & Extraction ────────────────────────────────────────────\n",
    "import io, json\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.storagelevel import StorageLevel          \n",
    "\n",
    "def _schema(kind: str) -> T.StructType:\n",
    "    \"\"\"\n",
    "    Construct a Spark StructType schema for either posts or comments.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kind : str\n",
    "        Type of file to read; either \"post\" or \"comment\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    StructType\n",
    "        A schema with fields common to both plus either 'title'/'selftext'\n",
    "        for posts or 'body' for comments.\n",
    "    \"\"\"\n",
    "    # Base fields common to both posts and comments\n",
    "    base = [T.StructField(c, T.StringType(), True)\n",
    "            for c in (\"id\", \"created_utc\", \"subreddit\")]\n",
    "    # Extend schema for submissions (posts)\n",
    "    if kind == \"post\":\n",
    "        base += [\n",
    "            T.StructField(\"title\",    T.StringType(), True),\n",
    "            T.StructField(\"selftext\", T.StringType(), True)\n",
    "        ]\n",
    "    # Extend schema for comments\n",
    "    else:\n",
    "        base.append(T.StructField(\"body\", T.StringType(), True))\n",
    "    return T.StructType(base)\n",
    "\n",
    "def _standardize(df, kind: str):\n",
    "    \"\"\"\n",
    "    Standardize DataFrame columns for downstream processing.\n",
    "\n",
    "    - Cast 'created_utc' to long\n",
    "    - Ensure common column names: id, created_utc, subreddit, title, selftext\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Input DataFrame with raw fields.\n",
    "    kind : str\n",
    "        \"post\" or \"comment\" indicating which transformation to apply.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Transformed and pruned DataFrame with a uniform column set.\n",
    "    \"\"\"\n",
    "    # For comments, rename 'body' → 'selftext' and add missing title column\n",
    "    if kind == \"comment\":\n",
    "        df = (df.withColumn(\"title\",    F.lit(None).cast(\"string\"))\n",
    "                .withColumn(\"selftext\", F.col(\"body\"))\n",
    "                .drop(\"body\"))\n",
    "    # Cast timestamp and select only the standardized columns\n",
    "    return (df.withColumn(\"created_utc\", F.col(\"created_utc\").cast(\"long\"))\n",
    "              .select(\"id\", \"created_utc\", \"subreddit\", \"title\", \"selftext\"))\n",
    "\n",
    "def read_zst(path: str, kind: str):\n",
    "    \"\"\"\n",
    "    Read a .zst-compressed JSON lines file into a standardized Spark DataFrame.\n",
    "\n",
    "    The function first attempts to use Spark's built-in Text reader. \n",
    "    If that fails, it falls back to Python-side streaming decompression using the\n",
    "    zstandard library.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        URI of the .zst file or files (e.g. \"s3://bucket/file.zst\").\n",
    "    kind : str\n",
    "        Either \"post\" (for submissions) or \"comment\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        A Spark DataFrame with columns [id, created_utc, subreddit, title, selftext].\n",
    "    \"\"\"\n",
    "    kind = kind.lower()\n",
    "    schema = _schema(kind)\n",
    "\n",
    "    # ------------ fast path : JVM unzip via spark.read.text ---------------\n",
    "    try:\n",
    "        # Read each line as raw text, parse JSON, and apply the schema\n",
    "        df = (spark.read\n",
    "                    .text(path)                             # Hadoop will decompress if codec is present\n",
    "                    .select(F.from_json(\"value\", schema).alias(\"j\"))\n",
    "                    .select(\"j.*\"))\n",
    "    except Exception:\n",
    "        # ---------- fallback : Python zstandard streaming decompression ----------\n",
    "        import zstandard as zstd, contextlib\n",
    "        need_cols = schema.names\n",
    "\n",
    "        def _decode(iterator):\n",
    "            \"\"\"\n",
    "            Decompress and parse each .zst file's bytes into JSON records.\n",
    "            Runs within each Spark partition.\n",
    "            \"\"\"\n",
    "            dctx = zstd.ZstdDecompressor()\n",
    "            for _, pds in iterator:\n",
    "                # 'pds' is PortableDataStream; open it as a file-like object\n",
    "                with contextlib.closing(pds.open()) as raw:\n",
    "                    # Stream-decompress and decode to text\n",
    "                    rdr = dctx.stream_reader(raw)\n",
    "                    txt = io.TextIOWrapper(rdr, encoding=\"utf-8\")\n",
    "                    # Parse each line of JSON\n",
    "                    for ln in txt:\n",
    "                        rec = json.loads(ln)\n",
    "                        # Return only the columns defined in the schema\n",
    "                        yield {k: rec.get(k) for k in need_cols}\n",
    "\n",
    "        # Read raw bytes in parallel, decode, and then create DataFrame with schema\n",
    "        rdd = (spark.sparkContext\n",
    "                 .binaryFiles(path, minPartitions=60)  # partition count for parallelism\n",
    "                 .mapPartitions(_decode))\n",
    "        df = spark.createDataFrame(rdd, schema)\n",
    "\n",
    "    # Standardize column names, types, and select only necessary columns\n",
    "    return _standardize(df, kind)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f02fee9",
   "metadata": {},
   "source": [
    "#### 1.3 Appending reddit wsb posts and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66598d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 88951309\n",
      "Parquet saved to s3a://wsb-research-data-shef-20250522/stage01_union_parquet/"
     ]
    }
   ],
   "source": [
    "# ─── Cell 3: Append posts and comments ────────────────────────\n",
    "# Define S3 bucket and prefix for raw data files\n",
    "BUCKET  = \"wsb-research-data-shef-20250522\"\n",
    "SRC_PRE = \"raw/wsb24\"\n",
    "\n",
    "# Read WallStreetBets submissions (.zst) into a DataFrame, tag rows as not comments\n",
    "posts = (read_zst(f\"s3://{BUCKET}/{SRC_PRE}/wallstreetbets_submissions.zst\",\"post\")\n",
    "           .withColumn(\"is_comment\", F.lit(False)))\n",
    "\n",
    "# Read WallStreetBets comments (.zst) into a DataFrame, tag rows as comments         \n",
    "coms  = (read_zst(f\"s3://{BUCKET}/{SRC_PRE}/wallstreetbets_comments.zst\",\"comment\")\n",
    "           .withColumn(\"is_comment\", F.lit(True)))\n",
    "\n",
    "\n",
    "# Union submissions and comments into a single DataFrame,\n",
    "# then repartition to 200 partitions for balanced parallelism,\n",
    "# and persist to disk-only storage to avoid repeated recomputation.\n",
    "union_df = (posts.unionByName(coms)\n",
    "                  .repartition(200)                # down to 200 partitions\n",
    "                  .persist(StorageLevel.DISK_ONLY))\n",
    "\n",
    "print(\"Total rows:\", union_df.count())\n",
    "\n",
    "# Define output path\n",
    "stage1 = f\"s3a://{BUCKET}/stage01_union_parquet/\"\n",
    "\n",
    "# Write the unified DataFrame out as Parquet\n",
    "(union_df.write.mode(\"overwrite\").parquet(stage1))\n",
    "\n",
    "# Confirm successful write\n",
    "print(\"Parquet saved to\", stage1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a668dee2",
   "metadata": {},
   "source": [
    "#### 1.4 Read CRSP (stock ticker information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bced19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasted CRSP tickers: 152518"
     ]
    }
   ],
   "source": [
    "# ─── Cell 4: Read CRSP & Broadcast ────────────────────────\n",
    "from pyspark.sql.functions import unix_timestamp, upper, col, broadcast\n",
    "\n",
    "# Load the CRSP ticker list from S3 into a Spark DataFrame:\n",
    "# - Read CSV with header\n",
    "# - Select and transform columns:\n",
    "#   • Uppercase the \"Ticker\" column for consistency\n",
    "#   • Parse SecInfoStartDt and SecInfoEndDt strings (\"yyyyMMdd\") into epoch seconds\n",
    "#   • Cast timestamps to long type and alias as start_ts and end_ts\n",
    "# - Drop any rows missing a ticker symbol\n",
    "crsp_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .csv(f\"s3a://{BUCKET}/crsp_ticker_list_unique.csv\")\n",
    "         .select(\n",
    "             upper(col(\"Ticker\")).alias(\"ticker\"),\n",
    "             unix_timestamp(col(\"SecInfoStartDt\"), \"yyyyMMdd\").cast(\"long\").alias(\"start_ts\"),\n",
    "             unix_timestamp(col(\"SecInfoEndDt\"),   \"yyyyMMdd\").cast(\"long\").alias(\"end_ts\")\n",
    "         )\n",
    "         .na.drop(subset=[\"ticker\"])\n",
    ")\n",
    "\n",
    "# Broadcast the CRSP DataFrame to all executors to optimize subsequent joins\n",
    "crsp_b = broadcast(crsp_df)\n",
    "\n",
    "# Print the number of valid tickers loaded\n",
    "print(\"Broadcasted CRSP tickers:\", crsp_df.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d90dd7",
   "metadata": {},
   "source": [
    "#### 1.5: Indentifying the stock ticker that the texts is describing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bacbe65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? Stage-2 Parquet written: s3a://wsb-research-data-shef-20250522/stage02_by_ticker/"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, explode, length, col, broadcast\n",
    "\n",
    "# ---------- Reddit ----------\n",
    "\n",
    "# Combine the title and selftext into a single string column 'raw_txt'\n",
    "posts2 = posts.withColumn(\n",
    "    \"raw_txt\", expr(\"concat_ws(' ', title, selftext)\")\n",
    ")\n",
    "\n",
    "# Extract all candidate ticker-like tokens matching $?[A-Z]{1,5} into an array 'cand'\n",
    "posts3 = posts2.withColumn(\n",
    "    \"cand\", expr(r\"regexp_extract_all(raw_txt, '(\\$?[A-Z]{1,5})\\b', 0)\")\n",
    ")\n",
    "\n",
    "# Explode the array of candidate tokens so each row has one 'tok_raw'\n",
    "exploded = posts3.select(\n",
    "    \"id\", \"created_utc\", \"subreddit\", \"is_comment\",\n",
    "    explode(\"cand\").alias(\"tok_raw\")\n",
    ")\n",
    "\n",
    "# Remove any leading '$' and uppercase the token to produce a normalized 'ticker'\n",
    "cleaned = exploded.withColumn(\n",
    "    \"ticker\", upper(expr(\"regexp_replace(tok_raw, '^\\\\$', '')\"))\n",
    ")\n",
    "\n",
    "# ---------- join + filter -------\n",
    "\n",
    "# Inner join with broadcasted CRSP table on 'ticker' to keep only valid symbols\n",
    "joined = cleaned.join(crsp_b, on=\"ticker\", how=\"inner\")\n",
    "\n",
    "# Filter rows where the post timestamp falls within the ticker's valid start/end dates\n",
    "# Also enforce that single-letter tickers must have originally started with '$'\n",
    "filtered = joined.filter(\n",
    "    (col(\"created_utc\") >= col(\"start_ts\")) &\n",
    "    (col(\"created_utc\") <= col(\"end_ts\")) &\n",
    "    (\n",
    "        (length(\"ticker\") >= 2) |\n",
    "        ((length(\"ticker\") == 1) & expr(\"tok_raw LIKE '$%'\"))\n",
    "    )\n",
    ").drop(\"start_ts\", \"end_ts\", \"tok_raw\")\n",
    "\n",
    "# Define output path for per-ticker Parquet files\n",
    "stage2 = f\"s3a://{BUCKET}/stage02_by_ticker/\"\n",
    "\n",
    "# Write out the filtered data:\n",
    "# - Repartition by 'ticker' for balanced writes\n",
    "# - Partition files by 'ticker' so each symbol lands in its own folder\n",
    "(filtered\n",
    "   .repartition(\"ticker\")\n",
    "   .write.mode(\"overwrite\")\n",
    "   .partitionBy(\"ticker\")\n",
    "   .parquet(stage2)\n",
    ")\n",
    "\n",
    "print(\"√ Stage-2 Parquet written:\", stage2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
